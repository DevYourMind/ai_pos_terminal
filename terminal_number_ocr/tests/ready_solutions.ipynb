{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сравнение готовых решений из интернета"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Загрузка библиотек"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import pytesseract\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import easyocr\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import onnx\n",
    "import onnxruntime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_images = Path('images')\n",
    "images = [str(path_images / x) for x in os.listdir(path_images)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EasyOCR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline: default EasyOCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Neither CUDA nor MPS are available - defaulting to CPU. Note: This module is much faster with a GPU.\n"
     ]
    }
   ],
   "source": [
    "reader = easyocr.Reader(['en'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [02:00<00:00, 12.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 7min 56s\n",
      "Wall time: 2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for filename in tqdm(images):\n",
    "    index = filename.split('\\\\')[1].split('.')[0]\n",
    "    # image = cv2.imread(filename) # read image\n",
    "    # if image.shape[0]<image.shape[1]:\n",
    "    #     image = cv2.rotate(image, cv2.ROTATE_90_COUNTERCLOCKWISE) # read image\n",
    "    image = Image.open(filename)\n",
    "    if image.size[0]>image.size[1]: image = image.rotate(270)\n",
    "    image = np.array(image)\n",
    "    results = reader.readtext(image, allowlist ='0123456789')\n",
    "    # # iterate on all results\n",
    "    # for res in results:\n",
    "    #     top_left = (int(res[0][0][0]), int(res[0][0][1])) # convert float to int\n",
    "    #     bottom_right = (int(res[0][2][0]), int(res[0][2][1])) # convert float to int\n",
    "    #     cv2.rectangle(image, top_left, bottom_right, (0, 255, 0), 3)\n",
    "    #     cv2.putText(image, res[1], (top_left[0], top_left[1]-10), cv2.FONT_HERSHEY_SIMPLEX, 1.2, (0, 0, 255), 2)\n",
    "    # cv2.imwrite(f'images/{index}_1.jpg', image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Terminal OD + EasyOCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_terminal = YOLO(\"../models/terminal_od.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:43<00:00,  4.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 1min 59s\n",
      "Wall time: 43.9 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dict_easyocr = {}\n",
    "for filename in tqdm(images):\n",
    "    image = Image.open(filename)\n",
    "    if image.size[0]>image.size[1]: image = image.rotate(270)\n",
    "    box_terminal = model_terminal(image, verbose=False)\n",
    "    x1, y1, x2, y2 = box_terminal[0].boxes.xyxy[0].numpy()\n",
    "    image = image.crop((x1, y1, x2, y2))\n",
    "    image = np.array(image)\n",
    "    results = reader.readtext(image, allowlist='0123456789')\n",
    "    dict_easyocr[filename] = results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.3"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(60+13) / 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Terminal OD + EasyOCR ONNX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/JaidedAI/EasyOCR/issues/746\n",
    "\n",
    "https://colab.research.google.com/drive/1pcoueUxhWFX5Ac6AA4paYDLgZMf819GT?usp=sharing#scrollTo=CvV_DDmohmBy\n",
    "\n",
    "https://github.com/JaidedAI/EasyOCR/blob/ca9f9b0ac081f2874a603a5614ddaf9de40ac339/easyocr/config.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_onnx_files = Path('onnx_files')\n",
    "if not os.path.exists(path_onnx_files): os.makedirs(path_onnx_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from easyocr_onnx import detection\n",
    "import torch\n",
    "\n",
    "model = detection.get_detector(trained_model='onnx_files/craft_mlt_25k.pth', device='cpu', quantize=False)\n",
    "\n",
    "# input_shape = (1, 3, 480, 640)\n",
    "# inputs = torch.ones(*input_shape)\n",
    "# input_names=['input']\n",
    "# output_names=['output']\n",
    "\n",
    "# dynamic_axes= {'input':{0:'batch_size', 2:'height', 3:'width'}, 'output':{0:'batch_size', 2:'height', 3:'width'}} #adding names for better debugging\n",
    "# torch.onnx.export(model, inputs, \"craft.onnx\", dynamic_axes=dynamic_axes, input_names=input_names, output_names=output_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime as rt\n",
    "import cv2\n",
    "import numpy as np\n",
    "from easyocr_onnx.craft_utils import getDetBoxes, adjustResultCoordinates\n",
    "from easyocr_onnx.imgproc import resize_aspect_ratio, normalizeMeanVariance\n",
    "from easyocr_onnx.utils import reformat_input, get_image_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Сокращенный код"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_image(image):\n",
    "    if type(image) == str:\n",
    "        img = cv2.imread(image)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    elif type(image) == bytes:\n",
    "        nparr = np.frombuffer(image, np.uint8)\n",
    "        img = cv2.imdecode(nparr, cv2.IMREAD_COLOR)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    elif type(image) == np.ndarray:\n",
    "        if len(image.shape) == 2:  # grayscale\n",
    "            img = cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)\n",
    "        elif len(image.shape) == 3 and image.shape[2] == 3:\n",
    "            img = image\n",
    "        elif len(image.shape) == 3 and image.shape[2] == 4:  # RGBAscale\n",
    "            img = image[:, :, :3]\n",
    "    return img\n",
    "\n",
    "\n",
    "def rectify_poly(img, poly):\n",
    "    # Use Affine transform\n",
    "    n = int(len(poly) / 2) - 1\n",
    "    width = 0\n",
    "    height = 0\n",
    "    for k in range(n):\n",
    "        box = np.float32([poly[k], poly[k + 1], poly[-k - 2], poly[-k - 1]])\n",
    "        width += int(\n",
    "            (np.linalg.norm(box[0] - box[1]) + np.linalg.norm(box[2] - box[3])) / 2\n",
    "        )\n",
    "        height += np.linalg.norm(box[1] - box[2])\n",
    "    width = int(width)\n",
    "    height = int(height / n)\n",
    "\n",
    "    output_img = np.zeros((height, width, 3), dtype=np.uint8)\n",
    "    width_step = 0\n",
    "    for k in range(n):\n",
    "        box = np.float32([poly[k], poly[k + 1], poly[-k - 2], poly[-k - 1]])\n",
    "        w = int((np.linalg.norm(box[0] - box[1]) + np.linalg.norm(box[2] - box[3])) / 2)\n",
    "\n",
    "        # Top triangle\n",
    "        pts1 = box[:3]\n",
    "        pts2 = np.float32(\n",
    "            [[width_step, 0], [width_step + w - 1, 0], [width_step + w - 1, height - 1]]\n",
    "        )\n",
    "        M = cv2.getAffineTransform(pts1, pts2)\n",
    "        warped_img = cv2.warpAffine(\n",
    "            img, M, (width, height), borderMode=cv2.BORDER_REPLICATE)\n",
    "        warped_mask = np.zeros((height, width, 3), dtype=np.uint8)\n",
    "        warped_mask = cv2.fillConvexPoly(warped_mask, np.int32(pts2), (1, 1, 1))\n",
    "        output_img[warped_mask == 1] = warped_img[warped_mask == 1]\n",
    "\n",
    "        # Bottom triangle\n",
    "        pts1 = np.vstack((box[0], box[2:]))\n",
    "        pts2 = np.float32(\n",
    "            [\n",
    "                [width_step, 0],\n",
    "                [width_step + w - 1, height - 1],\n",
    "                [width_step, height - 1],\n",
    "            ]\n",
    "        )\n",
    "        M = cv2.getAffineTransform(pts1, pts2)\n",
    "        warped_img = cv2.warpAffine(\n",
    "            img, M, (width, height), borderMode=cv2.BORDER_REPLICATE\n",
    "        )\n",
    "        warped_mask = np.zeros((height, width, 3), dtype=np.uint8)\n",
    "        warped_mask = cv2.fillConvexPoly(warped_mask, np.int32(pts2), (1, 1, 1))\n",
    "        cv2.line(\n",
    "            warped_mask, (width_step, 0), (width_step + w - 1, height - 1), (0, 0, 0), 1\n",
    "        )\n",
    "        output_img[warped_mask == 1] = warped_img[warped_mask == 1]\n",
    "\n",
    "        width_step += w\n",
    "    return output_img\n",
    "\n",
    "\n",
    "def crop_poly(image, poly):\n",
    "    # points should have 1*x*2  shape\n",
    "    if len(poly.shape) == 2:\n",
    "        poly = np.array([np.array(poly).astype(np.int32)])\n",
    "\n",
    "    # create mask with shape of image\n",
    "    mask = np.zeros(image.shape[0:2], dtype=np.uint8)\n",
    "\n",
    "    # method 1 smooth region\n",
    "    cv2.drawContours(mask, [poly], -1, (255, 255, 255), -1, cv2.LINE_AA)\n",
    "    # method 2 not so smooth region\n",
    "    # cv2.fillPoly(mask, points, (255))\n",
    "\n",
    "    # crop around poly\n",
    "    res = cv2.bitwise_and(image, image, mask=mask)\n",
    "    rect = cv2.boundingRect(poly)  # returns (x,y,w,h) of the rect\n",
    "    cropped = res[rect[1] : rect[1] + rect[3], rect[0] : rect[0] + rect[2]]\n",
    "    return cropped\n",
    "\n",
    "\n",
    "def export_detected_region(image, poly, rectify=True):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        image: full image\n",
    "        points: bbox or poly points        \n",
    "        rectify: rectify detected polygon by affine transform\n",
    "    \"\"\"\n",
    "    if rectify:\n",
    "        # rectify poly region\n",
    "        result_rgb = rectify_poly(image, poly)\n",
    "    else:\n",
    "        result_rgb = crop_poly(image, poly)\n",
    "    # export corpped region\n",
    "    result_bgr = cv2.cvtColor(result_rgb, cv2.COLOR_RGB2BGR)\n",
    "    return result_bgr\n",
    "\n",
    "\n",
    "import copy\n",
    "\n",
    "def export_detected_regions(\n",
    "    image,\n",
    "    regions,    \n",
    "    rectify: bool = False,\n",
    "):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        image: path to the image to be processed or numpy array or PIL image\n",
    "        regions: list of bboxes or polys        \n",
    "        rectify: rectify detected polygon by affine transform\n",
    "    \"\"\"\n",
    "    # read/convert image\n",
    "    image = read_image(image)\n",
    "    # deepcopy image so that original is not altered\n",
    "    image = copy.deepcopy(image)\n",
    "    # init exported file paths\n",
    "    exported_images = []\n",
    "    # export regions\n",
    "    for ind, region in enumerate(regions):\n",
    "        # get export path\n",
    "        #file_path = os.path.join(crops_dir, \"crop_\" + str(ind) + \".png\")\n",
    "        # export region\n",
    "        crop = export_detected_region(image, poly=region, rectify=rectify)\n",
    "        # note exported file path\n",
    "        exported_images.append(crop)\n",
    "    return exported_images\n",
    "\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from easyocr_onnx import recognition\n",
    "\n",
    "def recognizer_predict(converter, test_loader, batch_max_length,\\\n",
    "                       ignore_idx, char_group_idx, decoder = 'greedy', beamWidth= 5, device = 'cpu'):    \n",
    "    result = []\n",
    "    with torch.no_grad():\n",
    "        for image_tensors in test_loader:\n",
    "            batch_size = image_tensors.size(0)\n",
    "            image = image_tensors.to(device)\n",
    "            # For max length prediction\n",
    "            length_for_pred = torch.IntTensor([batch_max_length] * batch_size).to(device)\n",
    "            text_for_pred = torch.LongTensor(batch_size, batch_max_length + 1).fill_(0).to(device)\n",
    "\n",
    "            #preds = model(image, text_for_pred)\n",
    "\n",
    "            providers = ['CPUExecutionProvider']\n",
    "            session = rt.InferenceSession(\"onnx_files/recog.onnx\", providers=providers)\n",
    "            inputs = session.get_inputs()\n",
    "\n",
    "            inp = {inputs[0].name: image.numpy()}\n",
    "            preds = session.run(None, inp)\n",
    "\n",
    "            preds = torch.from_numpy(preds[0])\n",
    "\n",
    "            # Select max probabilty (greedy decoding) then decode index to character\n",
    "            preds_size = torch.IntTensor([preds.size(1)] * batch_size)\n",
    "\n",
    "            ######## filter ignore_char, rebalance\n",
    "            preds_prob = F.softmax(preds, dim=2)\n",
    "            preds_prob = preds_prob.cpu().detach().numpy()\n",
    "            preds_prob[:,:,ignore_idx] = 0.\n",
    "            pred_norm = preds_prob.sum(axis=2)\n",
    "            preds_prob = preds_prob/np.expand_dims(pred_norm, axis=-1)\n",
    "            preds_prob = torch.from_numpy(preds_prob).float().to(device)\n",
    "\n",
    "            if decoder=='greedy':\n",
    "                # Select max probabilty (greedy decoding) then decode index to character\n",
    "                _, preds_index = preds_prob.max(2)\n",
    "                preds_index = preds_index.view(-1)\n",
    "                preds_str = converter.decode_greedy(preds_index.data.cpu().detach().numpy(), preds_size.data)\n",
    "            elif decoder == 'beamsearch':\n",
    "                k = preds_prob.cpu().detach().numpy()\n",
    "                preds_str = converter.decode_beamsearch(k, beamWidth=beamWidth)\n",
    "            elif decoder == 'wordbeamsearch':\n",
    "                k = preds_prob.cpu().detach().numpy()\n",
    "                preds_str = converter.decode_wordbeamsearch(k, beamWidth=beamWidth)\n",
    "\n",
    "            preds_prob = preds_prob.cpu().detach().numpy()\n",
    "            values = preds_prob.max(axis=2)\n",
    "            indices = preds_prob.argmax(axis=2)\n",
    "            preds_max_prob = []\n",
    "            for v, i in zip(values, indices):\n",
    "                max_probs = v[i!=0]\n",
    "                if len(max_probs)>0:\n",
    "                    preds_max_prob.append(max_probs)\n",
    "                else:\n",
    "                    preds_max_prob.append(np.array([0]))\n",
    "\n",
    "            for pred, pred_max_prob in zip(preds_str, preds_max_prob):\n",
    "                confidence_score = recognition.custom_mean(pred_max_prob)\n",
    "                result.append([pred, confidence_score])\n",
    "    return result\n",
    "\n",
    "def get_text(character, imgH, imgW, converter, image_list,\\\n",
    "             ignore_char = '',decoder = 'greedy', beamWidth =5, batch_size=1, contrast_ths=0.1,\\\n",
    "             adjust_contrast=0.5, filter_ths = 0.003, workers = 1, device = 'cpu'):\n",
    "    batch_max_length = int(imgW/10)\n",
    "\n",
    "    char_group_idx = {}\n",
    "    ignore_idx = []\n",
    "    for char in ignore_char:\n",
    "        try: ignore_idx.append(character.index(char)+1)\n",
    "        except: pass\n",
    "\n",
    "    coord = [item[0] for item in image_list]\n",
    "    img_list = [item[1] for item in image_list]\n",
    "    AlignCollate_normal = recognition.AlignCollate(imgH=imgH, imgW=imgW, keep_ratio_with_pad=True)\n",
    "    test_data = recognition.ListDataset(img_list)\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        test_data, batch_size=batch_size, shuffle=False,\n",
    "        num_workers=int(workers), collate_fn=AlignCollate_normal, pin_memory=True)\n",
    "\n",
    "    # predict first round\n",
    "    result1 = recognizer_predict(converter, test_loader,batch_max_length,\\\n",
    "                                 ignore_idx, char_group_idx, decoder, beamWidth, device = device)\n",
    "\n",
    "    # predict second round\n",
    "    low_confident_idx = [i for i,item in enumerate(result1) if (item[1] < contrast_ths)]\n",
    "    if len(low_confident_idx) > 0:\n",
    "        img_list2 = [img_list[i] for i in low_confident_idx]\n",
    "        AlignCollate_contrast = recognition.AlignCollate(imgH=imgH, imgW=imgW, keep_ratio_with_pad=True, adjust_contrast=adjust_contrast)\n",
    "        test_data = recognition.ListDataset(img_list2)\n",
    "        test_loader = torch.utils.data.DataLoader(\n",
    "                        test_data, batch_size=batch_size, shuffle=False,\n",
    "                        num_workers=int(workers), collate_fn=AlignCollate_contrast, pin_memory=True)\n",
    "        result2 = recognizer_predict(converter, test_loader, batch_max_length,\\\n",
    "                                     ignore_idx, char_group_idx, decoder, beamWidth, device = device)\n",
    "\n",
    "    result = []\n",
    "    for i, zipped in enumerate(zip(coord, result1)):\n",
    "        box, pred1 = zipped\n",
    "        if i in low_confident_idx:\n",
    "            pred2 = result2[low_confident_idx.index(i)]\n",
    "            if pred1[1]>pred2[1]:\n",
    "                result.append( (box, pred1[0], pred1[1]) )\n",
    "            else:\n",
    "                result.append( (box, pred2[0], pred2[1]) )\n",
    "        else:\n",
    "            result.append( (box, pred1[0], pred1[1]) )\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# character = \"0123456789!\\\"#$%&'()*+,-./:;<=>?@[\\\\]^_`{|}~ €ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\"\n",
    "# symbol = \"0123456789!\\\"#$%&'()*+,-./:;<=>?@[\\\\]^_`{|}~ €\"\n",
    "# recog_network = 'generation2'\n",
    "# model_path = \"onnx_files/english_g2.pth\"\n",
    "# separator_list = {}\n",
    "# cyrillic_lang_list = ['en']\n",
    "# package_dir = os.path.dirname(recognition.__file__)\n",
    "# network_params = {\n",
    "#     'input_channel': 1,\n",
    "#     'output_channel': 256,\n",
    "#     'hidden_size': 256\n",
    "#     }\n",
    "# dict_list = {}\n",
    "# for lang in cyrillic_lang_list:\n",
    "#     dict_list[lang] = os.path.join(package_dir, 'dict', lang + \".txt\")\n",
    "\n",
    "# model, converter = recognition.get_recognizer(recog_network=recog_network, network_params=network_params, character=character, separator_list=separator_list, dict_list=dict_list, model_path=model_path, device='cpu', quantize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size = 1\n",
    "# num_channels = 1\n",
    "# image_height = imgH = 64\n",
    "# image_width = 128\n",
    "# image_input_shape = (batch_size, 1, image_height, image_width)\n",
    "# image_input = torch.ones(*image_input_shape)\n",
    "# max_text_length = 10\n",
    "# text_input_shape = (batch_size, max_text_length)\n",
    "# text_input = torch.ones(*text_input_shape)\n",
    "# input_names=['image_input', 'text_input']\n",
    "# output_names=['output']\n",
    "# dynamic_axes = {\"image_input\": {0: \"batch_size\", 3: \"width\"}, \"text_input\": {0: \"batch_size\"}}\n",
    "# opset_version = 12\n",
    "\n",
    "# torch.onnx.export(model, (image_input, text_input), \"onnx_files/recog.onnx\", \n",
    "#                   input_names=input_names, output_names=output_names, \n",
    "#                   dynamic_axes=dynamic_axes, opset_version=opset_version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "character = '0123456789!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~ €₽ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyzАБВГДЕЁЖЗИЙКЛМНОПРСТУФХЦЧШЩЪЫЬЭЮЯабвгдеёжзийклмнопрстуфхцчшщъыьэюяЂђЃѓЄєІіЇїЈјЉљЊњЋћЌќЎўЏџҐґҒғҚқҮүҲҳҶҷӀӏӢӣӨөӮӯ'\n",
    "symbol = '0123456789!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~ €₽'\n",
    "model_path = \"cyrillic_g2.pth\"\n",
    "separator_list = {}\n",
    "cyrillic_lang_list = ['ru','rs_cyrillic','be','bg','uk','mn','abq','ady','kbd',\\\n",
    "                      'ava','dar','inh','che','lbe','lez','tab','tjk', 'en']\n",
    "package_dir = os.path.dirname(recognition.__file__)\n",
    "\n",
    "dict_list = {}\n",
    "for lang in cyrillic_lang_list:\n",
    "    dict_list[lang] = os.path.join(package_dir, 'dict', lang + \".txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "converter = recognition.CTCLabelConverter(character, separator_list, dict_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ONNX Runtime session and load model\n",
    "providers = ['CPUExecutionProvider']\n",
    "session = rt.InferenceSession(\"onnx_files/craft.onnx\", providers=providers)\n",
    "input_name = session.get_inputs()[0].name\n",
    "\n",
    "batch_size = 1\n",
    "num_channels = 1\n",
    "image_height = imgH = 64\n",
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_easyocr_results(filename):\n",
    "    image = Image.open(filename)\n",
    "    if image.size[0]>image.size[1]: image = image.rotate(270)\n",
    "    box_terminal = model_terminal(image, verbose=False)\n",
    "    x1, y1, x2, y2 = box_terminal[0].boxes.xyxy[0].numpy()\n",
    "    image = image.crop((x1, y1, x2, y2))\n",
    "    img, _ = reformat_input(np.array(image))\n",
    "\n",
    "    # Resize and normalize input image\n",
    "    img_resized, target_ratio, size_heatmap = resize_aspect_ratio(img, 512, interpolation=cv2.INTER_LINEAR, mag_ratio=1.)\n",
    "    ratio_h = ratio_w = 1 / target_ratio\n",
    "    x = normalizeMeanVariance(img_resized)\n",
    "    x = torch.from_numpy(x).permute(2, 0, 1).unsqueeze(0)\n",
    "    # Prepare input tensor for inference\n",
    "    inp = {input_name: x.numpy()}\n",
    "    # Run inference and get output\n",
    "    y, _ = session.run(None, inp)\n",
    "    # Extract score and link maps\n",
    "    score_text = y[0, :, :, 0]\n",
    "    score_link = y[0, :, :, 1]\n",
    "    # Post-processing to obtain bounding boxes and polygons\n",
    "    boxes, _, _ = getDetBoxes(score_text, score_link, 0.5, 0.4, 0.4)\n",
    "    boxes = adjustResultCoordinates(boxes, ratio_w, ratio_h)\n",
    "\n",
    "    result = []\n",
    "    lang_char = symbol[:10]\n",
    "    ignore_char = ''.join(set(character)-set(lang_char))  \n",
    "\n",
    "    crops = export_detected_regions(image=img, regions=boxes, rectify=True)\n",
    "    for crop in crops:\n",
    "        img, img_cv_grey = reformat_input(crop)\n",
    "        y_max, x_max = img_cv_grey.shape\n",
    "        horizontal_list = [[0, x_max, 0, y_max]]\n",
    "        for bbox in horizontal_list:\n",
    "            h_list = [bbox]\n",
    "            f_list = []\n",
    "            image_list, max_width = get_image_list(h_list, f_list, img_cv_grey, model_height=64) # 64 is default value\n",
    "            # result0 = get_text(character, imgH, int(max_width), converter, image_list,\\\n",
    "            #                             ignore_char, 'greedy', beamWidth=5, batch_size=batch_size, contrast_ths=0.1, adjust_contrast=0.5, filter_ths=0.003,\\\n",
    "            #                             workers=0, device=device)\n",
    "            result0 = get_text(character, imgH, int(max_width), converter, image_list,\\\n",
    "                                        ignore_char, 'greedy', batch_size=batch_size, \\\n",
    "                                        workers=0, device=device)\n",
    "            result += result0\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:18<00:00,  1.89s/it]\n"
     ]
    }
   ],
   "source": [
    "dict_results_onnx = {} \n",
    "for filename in tqdm(images):\n",
    "    dict_results_onnx[filename] = get_easyocr_results(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "lists_onnx = []\n",
    "for row in list(dict_results_onnx.values()):\n",
    "    lists_onnx.append([x[1] for x in row])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "lists_easyocr = []\n",
    "for row in list(dict_easyocr.values()):\n",
    "    lists_easyocr.append([x[1] for x in row])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\dimaz\\\\AppData\\\\Local\\\\Packages\\\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\\\LocalCache\\\\local-packages\\\\Python310\\\\site-packages\\\\easyocr_onnx\\\\__init__.py'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import easyocr_onnx\n",
    "easyocr_onnx.__file__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_onnx = pd.DataFrame({\n",
    "    'filename': list(dict_results_onnx.keys()),\n",
    "    'ocr_rows_onnx': lists_onnx})\n",
    "df_easyocr = pd.DataFrame({\n",
    "    'filename': list(dict_easyocr.keys()),\n",
    "    'ocr_rows_easyocr': lists_easyocr\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result = df_onnx.merge(df_easyocr, left_on='filename', right_on='filename')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_tids = ['30398560', '30395549',\n",
    " '30374544', '30370954',\n",
    " '30321657', '30372393',\n",
    " '30398967', '30370910',\n",
    " '30398363', '30409949'\n",
    " ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_flags = []\n",
    "for i, tid in enumerate(correct_tids):\n",
    "    onnx_flags.append(tid in df_result['ocr_rows_onnx'][i])\n",
    "\n",
    "easyocr_flags = []\n",
    "for i, tid in enumerate(correct_tids):\n",
    "    easyocr_flags.append(tid in df_result['ocr_rows_easyocr'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[True, True, True, True, True, True, False, True, False, False]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onnx_flags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[True, True, True, True, True, True, True, True, False, True]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "easyocr_flags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(onnx_flags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(easyocr_flags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filename = 'images/8.jpg'\n",
    "# image = Image.open(filename)\n",
    "# if image.size[0]>image.size[1]: image = image.rotate(270)\n",
    "# box_terminal = model_terminal(image, verbose=False)\n",
    "# x1, y1, x2, y2 = box_terminal[0].boxes.xyxy[0].numpy()\n",
    "# image = image.crop((x1, y1, x2, y2))\n",
    "# img, _ = reformat_input(np.array(image))\n",
    "\n",
    "# # Resize and normalize input image\n",
    "# img_resized, target_ratio, size_heatmap = resize_aspect_ratio(img, 512, interpolation=cv2.INTER_LINEAR, mag_ratio=1.)\n",
    "# ratio_h = ratio_w = 1 / target_ratio\n",
    "# x = normalizeMeanVariance(img_resized)\n",
    "# x = torch.from_numpy(x).permute(2, 0, 1).unsqueeze(0)\n",
    "# # Prepare input tensor for inference\n",
    "# inp = {input_name: x.numpy()}\n",
    "# # Run inference and get output\n",
    "# y, _ = session.run(None, inp)\n",
    "# # Extract score and link maps\n",
    "# score_text = y[0, :, :, 0]\n",
    "# score_link = y[0, :, :, 1]\n",
    "# # Post-processing to obtain bounding boxes and polygons\n",
    "# boxes, polys, mapper = getDetBoxes(score_text, score_link, 0.5, 0.4, 0.4)\n",
    "# boxes = adjustResultCoordinates(boxes, ratio_w, ratio_h)\n",
    "\n",
    "# result = []\n",
    "# lang_char = symbol[:10]\n",
    "# ignore_char = ''.join(set(character)-set(lang_char))  \n",
    "\n",
    "# crops = export_detected_regions(image=img, regions=boxes, rectify=True)\n",
    "# for crop in crops:\n",
    "#     img, img_cv_grey = reformat_input(crop)\n",
    "#     y_max, x_max = img_cv_grey.shape\n",
    "#     horizontal_list = [[0, x_max, 0, y_max]]\n",
    "#     for bbox in horizontal_list:\n",
    "#         h_list = [bbox]\n",
    "#         f_list = []\n",
    "#         image_list, max_width = get_image_list(h_list, f_list, img_cv_grey, model_height=64) # 64 is default value\n",
    "#         result0 = get_text(character, imgH, int(max_width), converter, image_list,\\\n",
    "#                                     ignore_char, 'greedy', beamWidth=5, batch_size=batch_size, contrast_ths=0.1, adjust_contrast=0.5, filter_ths=0.003,\\\n",
    "#                                     workers=0, device=device)\n",
    "#         result += result0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# char = []\n",
    "# for i in range(len(result)):\n",
    "#     char.append(result[i][1])\n",
    "# char"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Рабочий ONNX скрипт"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # im = Image.open('images/10.jpg') #.rotate(270)\n",
    "# filename = 'images/1.jpg'\n",
    "# image = Image.open(filename)\n",
    "# if image.size[0]>image.size[1]: image = image.rotate(270)\n",
    "# box_terminal = model_terminal(image, verbose=False)\n",
    "# x1, y1, x2, y2 = box_terminal[0].boxes.xyxy[0].numpy()\n",
    "# image = image.crop((x1, y1, x2, y2))\n",
    "# img, _ = reformat_input(np.array(image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Resize and normalize input image\n",
    "# img_resized, target_ratio, size_heatmap = resize_aspect_ratio(img, 512, interpolation=cv2.INTER_LINEAR, mag_ratio=1.)\n",
    "# ratio_h = ratio_w = 1 / target_ratio\n",
    "# x = normalizeMeanVariance(img_resized)\n",
    "# x = torch.from_numpy(x).permute(2, 0, 1).unsqueeze(0)\n",
    "\n",
    "# # Create ONNX Runtime session and load model\n",
    "# providers = ['CPUExecutionProvider']\n",
    "# session = rt.InferenceSession(\"onnx_files/craft.onnx\", providers=providers)\n",
    "# input_name = session.get_inputs()[0].name\n",
    "\n",
    "# # Prepare input tensor for inference\n",
    "# inp = {input_name: x.numpy()}\n",
    "\n",
    "# # Run inference and get output\n",
    "# y, _ = session.run(None, inp)\n",
    "\n",
    "# # Extract score and link maps\n",
    "# score_text = y[0, :, :, 0]\n",
    "# score_link = y[0, :, :, 1]\n",
    "\n",
    "# # Post-processing to obtain bounding boxes and polygons\n",
    "# boxes, polys, mapper = getDetBoxes(score_text, score_link, 0.5, 0.4, 0.4)\n",
    "# boxes = adjustResultCoordinates(boxes, ratio_w, ratio_h)\n",
    "# # polys = adjustResultCoordinates(polys, ratio_w, ratio_h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def read_image(image):\n",
    "#     if type(image) == str:\n",
    "#         img = cv2.imread(image)\n",
    "#         img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "#     elif type(image) == bytes:\n",
    "#         nparr = np.frombuffer(image, np.uint8)\n",
    "#         img = cv2.imdecode(nparr, cv2.IMREAD_COLOR)\n",
    "#         img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "#     elif type(image) == np.ndarray:\n",
    "#         if len(image.shape) == 2:  # grayscale\n",
    "#             img = cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)\n",
    "#         elif len(image.shape) == 3 and image.shape[2] == 3:\n",
    "#             img = image\n",
    "#         elif len(image.shape) == 3 and image.shape[2] == 4:  # RGBAscale\n",
    "#             img = image[:, :, :3]\n",
    "#     return img\n",
    "\n",
    "\n",
    "# def rectify_poly(img, poly):\n",
    "#     # Use Affine transform\n",
    "#     n = int(len(poly) / 2) - 1\n",
    "#     width = 0\n",
    "#     height = 0\n",
    "#     for k in range(n):\n",
    "#         box = np.float32([poly[k], poly[k + 1], poly[-k - 2], poly[-k - 1]])\n",
    "#         width += int(\n",
    "#             (np.linalg.norm(box[0] - box[1]) + np.linalg.norm(box[2] - box[3])) / 2\n",
    "#         )\n",
    "#         height += np.linalg.norm(box[1] - box[2])\n",
    "#     width = int(width)\n",
    "#     height = int(height / n)\n",
    "\n",
    "#     output_img = np.zeros((height, width, 3), dtype=np.uint8)\n",
    "#     width_step = 0\n",
    "#     for k in range(n):\n",
    "#         box = np.float32([poly[k], poly[k + 1], poly[-k - 2], poly[-k - 1]])\n",
    "#         w = int((np.linalg.norm(box[0] - box[1]) + np.linalg.norm(box[2] - box[3])) / 2)\n",
    "\n",
    "#         # Top triangle\n",
    "#         pts1 = box[:3]\n",
    "#         pts2 = np.float32(\n",
    "#             [[width_step, 0], [width_step + w - 1, 0], [width_step + w - 1, height - 1]]\n",
    "#         )\n",
    "#         M = cv2.getAffineTransform(pts1, pts2)\n",
    "#         warped_img = cv2.warpAffine(\n",
    "#             img, M, (width, height), borderMode=cv2.BORDER_REPLICATE\n",
    "#         )\n",
    "#         warped_mask = np.zeros((height, width, 3), dtype=np.uint8)\n",
    "#         warped_mask = cv2.fillConvexPoly(warped_mask, np.int32(pts2), (1, 1, 1))\n",
    "#         output_img[warped_mask == 1] = warped_img[warped_mask == 1]\n",
    "\n",
    "#         # Bottom triangle\n",
    "#         pts1 = np.vstack((box[0], box[2:]))\n",
    "#         pts2 = np.float32(\n",
    "#             [\n",
    "#                 [width_step, 0],\n",
    "#                 [width_step + w - 1, height - 1],\n",
    "#                 [width_step, height - 1],\n",
    "#             ]\n",
    "#         )\n",
    "#         M = cv2.getAffineTransform(pts1, pts2)\n",
    "#         warped_img = cv2.warpAffine(\n",
    "#             img, M, (width, height), borderMode=cv2.BORDER_REPLICATE\n",
    "#         )\n",
    "#         warped_mask = np.zeros((height, width, 3), dtype=np.uint8)\n",
    "#         warped_mask = cv2.fillConvexPoly(warped_mask, np.int32(pts2), (1, 1, 1))\n",
    "#         cv2.line(\n",
    "#             warped_mask, (width_step, 0), (width_step + w - 1, height - 1), (0, 0, 0), 1\n",
    "#         )\n",
    "#         output_img[warped_mask == 1] = warped_img[warped_mask == 1]\n",
    "\n",
    "#         width_step += w\n",
    "#     return output_img\n",
    "\n",
    "\n",
    "# def crop_poly(image, poly):\n",
    "#     # points should have 1*x*2  shape\n",
    "#     if len(poly.shape) == 2:\n",
    "#         poly = np.array([np.array(poly).astype(np.int32)])\n",
    "\n",
    "#     # create mask with shape of image\n",
    "#     mask = np.zeros(image.shape[0:2], dtype=np.uint8)\n",
    "\n",
    "#     # method 1 smooth region\n",
    "#     cv2.drawContours(mask, [poly], -1, (255, 255, 255), -1, cv2.LINE_AA)\n",
    "#     # method 2 not so smooth region\n",
    "#     # cv2.fillPoly(mask, points, (255))\n",
    "\n",
    "#     # crop around poly\n",
    "#     res = cv2.bitwise_and(image, image, mask=mask)\n",
    "#     rect = cv2.boundingRect(poly)  # returns (x,y,w,h) of the rect\n",
    "#     cropped = res[rect[1] : rect[1] + rect[3], rect[0] : rect[0] + rect[2]]\n",
    "#     return cropped\n",
    "\n",
    "\n",
    "# def export_detected_region(image, poly, rectify=True):\n",
    "#     \"\"\"\n",
    "#     Arguments:\n",
    "#         image: full image\n",
    "#         points: bbox or poly points        \n",
    "#         rectify: rectify detected polygon by affine transform\n",
    "#     \"\"\"\n",
    "#     if rectify:\n",
    "#         # rectify poly region\n",
    "#         result_rgb = rectify_poly(image, poly)\n",
    "#     else:\n",
    "#         result_rgb = crop_poly(image, poly)\n",
    "#     # export corpped region\n",
    "#     result_bgr = cv2.cvtColor(result_rgb, cv2.COLOR_RGB2BGR)\n",
    "#     return result_bgr\n",
    "\n",
    "\n",
    "# import copy\n",
    "\n",
    "# def export_detected_regions(\n",
    "#     image,\n",
    "#     regions,    \n",
    "#     rectify: bool = False,\n",
    "# ):\n",
    "#     \"\"\"\n",
    "#     Arguments:\n",
    "#         image: path to the image to be processed or numpy array or PIL image\n",
    "#         regions: list of bboxes or polys        \n",
    "#         rectify: rectify detected polygon by affine transform\n",
    "#     \"\"\"\n",
    "#     # read/convert image\n",
    "#     image = read_image(image)\n",
    "#     # deepcopy image so that original is not altered\n",
    "#     image = copy.deepcopy(image)\n",
    "#     # init exported file paths\n",
    "#     exported_images = []\n",
    "#     # export regions\n",
    "#     for ind, region in enumerate(regions):\n",
    "#         # get export path\n",
    "#         #file_path = os.path.join(crops_dir, \"crop_\" + str(ind) + \".png\")\n",
    "#         # export region\n",
    "#         crop = export_detected_region(image, poly=region, rectify=rectify)\n",
    "#         # note exported file path\n",
    "#         exported_images.append(crop)\n",
    "#     return exported_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# crops = export_detected_regions(\n",
    "#     image=img,\n",
    "#     regions=boxes,\n",
    "#     rectify=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch.nn.functional as F\n",
    "# from easyocr_onnx import recognition\n",
    "\n",
    "# def recognizer_predict(converter, test_loader, batch_max_length,\\\n",
    "#                        ignore_idx, char_group_idx, decoder = 'greedy', beamWidth= 5, device = 'cpu'):    \n",
    "#     result = []\n",
    "#     with torch.no_grad():\n",
    "#         for image_tensors in test_loader:\n",
    "#             batch_size = image_tensors.size(0)\n",
    "#             image = image_tensors.to(device)\n",
    "#             # For max length prediction\n",
    "#             length_for_pred = torch.IntTensor([batch_max_length] * batch_size).to(device)\n",
    "#             text_for_pred = torch.LongTensor(batch_size, batch_max_length + 1).fill_(0).to(device)\n",
    "\n",
    "#             #preds = model(image, text_for_pred)\n",
    "\n",
    "#             providers = ['CPUExecutionProvider']\n",
    "#             session = rt.InferenceSession(\"onnx_files/recog.onnx\", providers=providers)\n",
    "#             inputs = session.get_inputs()\n",
    "\n",
    "#             inp = {inputs[0].name: image.numpy()}\n",
    "#             preds = session.run(None, inp)\n",
    "\n",
    "#             preds = torch.from_numpy(preds[0])\n",
    "\n",
    "#             # Select max probabilty (greedy decoding) then decode index to character\n",
    "#             preds_size = torch.IntTensor([preds.size(1)] * batch_size)\n",
    "\n",
    "#             ######## filter ignore_char, rebalance\n",
    "#             preds_prob = F.softmax(preds, dim=2)\n",
    "#             preds_prob = preds_prob.cpu().detach().numpy()\n",
    "#             preds_prob[:,:,ignore_idx] = 0.\n",
    "#             pred_norm = preds_prob.sum(axis=2)\n",
    "#             preds_prob = preds_prob/np.expand_dims(pred_norm, axis=-1)\n",
    "#             preds_prob = torch.from_numpy(preds_prob).float().to(device)\n",
    "\n",
    "#             if decoder=='greedy':\n",
    "#                 # Select max probabilty (greedy decoding) then decode index to character\n",
    "#                 _, preds_index = preds_prob.max(2)\n",
    "#                 preds_index = preds_index.view(-1)\n",
    "#                 preds_str = converter.decode_greedy(preds_index.data.cpu().detach().numpy(), preds_size.data)\n",
    "#             elif decoder == 'beamsearch':\n",
    "#                 k = preds_prob.cpu().detach().numpy()\n",
    "#                 preds_str = converter.decode_beamsearch(k, beamWidth=beamWidth)\n",
    "#             elif decoder == 'wordbeamsearch':\n",
    "#                 k = preds_prob.cpu().detach().numpy()\n",
    "#                 preds_str = converter.decode_wordbeamsearch(k, beamWidth=beamWidth)\n",
    "\n",
    "#             preds_prob = preds_prob.cpu().detach().numpy()\n",
    "#             values = preds_prob.max(axis=2)\n",
    "#             indices = preds_prob.argmax(axis=2)\n",
    "#             preds_max_prob = []\n",
    "#             for v, i in zip(values, indices):\n",
    "#                 max_probs = v[i!=0]\n",
    "#                 if len(max_probs)>0:\n",
    "#                     preds_max_prob.append(max_probs)\n",
    "#                 else:\n",
    "#                     preds_max_prob.append(np.array([0]))\n",
    "\n",
    "#             for pred, pred_max_prob in zip(preds_str, preds_max_prob):\n",
    "#                 confidence_score = recognition.custom_mean(pred_max_prob)\n",
    "#                 result.append([pred, confidence_score])\n",
    "#     return result\n",
    "\n",
    "# def get_text(character, imgH, imgW, converter, image_list,\\\n",
    "#              ignore_char = '',decoder = 'greedy', beamWidth =5, batch_size=1, contrast_ths=0.1,\\\n",
    "#              adjust_contrast=0.5, filter_ths = 0.003, workers = 1, device = 'cpu'):\n",
    "#     batch_max_length = int(imgW/10)\n",
    "\n",
    "#     char_group_idx = {}\n",
    "#     ignore_idx = []\n",
    "#     for char in ignore_char:\n",
    "#         try: ignore_idx.append(character.index(char)+1)\n",
    "#         except: pass\n",
    "\n",
    "#     coord = [item[0] for item in image_list]\n",
    "#     img_list = [item[1] for item in image_list]\n",
    "#     AlignCollate_normal = recognition.AlignCollate(imgH=imgH, imgW=imgW, keep_ratio_with_pad=True)\n",
    "#     test_data = recognition.ListDataset(img_list)\n",
    "#     test_loader = torch.utils.data.DataLoader(\n",
    "#         test_data, batch_size=batch_size, shuffle=False,\n",
    "#         num_workers=int(workers), collate_fn=AlignCollate_normal, pin_memory=True)\n",
    "\n",
    "#     # predict first round\n",
    "#     result1 = recognizer_predict(converter, test_loader,batch_max_length,\\\n",
    "#                                  ignore_idx, char_group_idx, decoder, beamWidth, device = device)\n",
    "\n",
    "#     # predict second round\n",
    "#     low_confident_idx = [i for i,item in enumerate(result1) if (item[1] < contrast_ths)]\n",
    "#     if len(low_confident_idx) > 0:\n",
    "#         img_list2 = [img_list[i] for i in low_confident_idx]\n",
    "#         AlignCollate_contrast = recognition.AlignCollate(imgH=imgH, imgW=imgW, keep_ratio_with_pad=True, adjust_contrast=adjust_contrast)\n",
    "#         test_data = recognition.ListDataset(img_list2)\n",
    "#         test_loader = torch.utils.data.DataLoader(\n",
    "#                         test_data, batch_size=batch_size, shuffle=False,\n",
    "#                         num_workers=int(workers), collate_fn=AlignCollate_contrast, pin_memory=True)\n",
    "#         result2 = recognizer_predict(converter, test_loader, batch_max_length,\\\n",
    "#                                      ignore_idx, char_group_idx, decoder, beamWidth, device = device)\n",
    "\n",
    "#     result = []\n",
    "#     for i, zipped in enumerate(zip(coord, result1)):\n",
    "#         box, pred1 = zipped\n",
    "#         if i in low_confident_idx:\n",
    "#             pred2 = result2[low_confident_idx.index(i)]\n",
    "#             if pred1[1]>pred2[1]:\n",
    "#                 result.append( (box, pred1[0], pred1[1]) )\n",
    "#             else:\n",
    "#                 result.append( (box, pred2[0], pred2[1]) )\n",
    "#         else:\n",
    "#             result.append( (box, pred1[0], pred1[1]) )\n",
    "#     return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# character = '0123456789!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~ €₽ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyzАБВГДЕЁЖЗИЙКЛМНОПРСТУФХЦЧШЩЪЫЬЭЮЯабвгдеёжзийклмнопрстуфхцчшщъыьэюяЂђЃѓЄєІіЇїЈјЉљЊњЋћЌќЎўЏџҐґҒғҚқҮүҲҳҶҷӀӏӢӣӨөӮӯ'\n",
    "# symbol = '0123456789!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~ €₽'\n",
    "# model_path = \"cyrillic_g2.pth\"\n",
    "# separator_list = {}\n",
    "# cyrillic_lang_list = ['ru','rs_cyrillic','be','bg','uk','mn','abq','ady','kbd',\\\n",
    "#                       'ava','dar','inh','che','lbe','lez','tab','tjk', 'en']\n",
    "# package_dir = os.path.dirname(recognition.__file__)\n",
    "\n",
    "# dict_list = {}\n",
    "# for lang in cyrillic_lang_list:\n",
    "#     dict_list[lang] = os.path.join(package_dir, 'dict', lang + \".txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from easyocr_onnx.utils import reformat_input, get_image_list\n",
    "\n",
    "\n",
    "# batch_size = 1\n",
    "# num_channels = 1\n",
    "# image_height = imgH = 64\n",
    "\n",
    "# result = []\n",
    "# # read image\n",
    "# converter = recognition.CTCLabelConverter(character, separator_list, dict_list)\n",
    "# device = 'cpu'\n",
    "\n",
    "# lang_char = []\n",
    "# for lang in cyrillic_lang_list:\n",
    "#   char_file = os.path.join(package_dir, 'character', lang + \"_char.txt\")\n",
    "#   with open(char_file, \"r\", encoding = \"utf-8-sig\") as input_file:\n",
    "#     char_list =  input_file.read().splitlines()\n",
    "#   lang_char += char_list\n",
    "# lang_char = set(lang_char).union(set(symbol))\n",
    "# # lang_char = set(lang_char).union(set(symbol[:10]))\n",
    "# lang_char = symbol[:10]\n",
    "# ignore_char = ''.join(set(character)-set(lang_char))  \n",
    "\n",
    "# for crop in crops:\n",
    "#   img, img_cv_grey = reformat_input(crop)\n",
    "#   y_max, x_max = img_cv_grey.shape\n",
    "#   horizontal_list = [[0, x_max, 0, y_max]]\n",
    "#   for bbox in horizontal_list:\n",
    "#       h_list = [bbox]\n",
    "#       f_list = []\n",
    "#       image_list, max_width = get_image_list(h_list, f_list, img_cv_grey, model_height=64) # 64 is default value\n",
    "#       result0 = get_text(character, imgH, int(max_width), converter, image_list,\\\n",
    "#                                 ignore_char, 'greedy', beamWidth = 5, batch_size=batch_size, contrast_ths = 0.1, adjust_contrast = 0.5, filter_ths = 0.003,\\\n",
    "#                                 workers = 0, device = device)\n",
    "#       result += result0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# char = []\n",
    "# for i in range(len(result)):\n",
    "#     char.append(result[i][1])\n",
    "# char"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Terminal OD + ONNX en_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_terminal = YOLO(\"../models/terminal_od.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import onnxruntime as rt\n",
    "from easyocr_onnx.craft_utils import getDetBoxes, adjustResultCoordinates\n",
    "from easyocr_onnx.imgproc import resize_aspect_ratio, normalizeMeanVariance\n",
    "from easyocr_onnx.utils import reformat_input, get_image_list\n",
    "from easyocr_onnx import recognition\n",
    "\n",
    "def read_image(image):\n",
    "    if type(image) == str:\n",
    "        img = cv2.imread(image)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    elif type(image) == bytes:\n",
    "        nparr = np.frombuffer(image, np.uint8)\n",
    "        img = cv2.imdecode(nparr, cv2.IMREAD_COLOR)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    elif type(image) == np.ndarray:\n",
    "        if len(image.shape) == 2:  # grayscale\n",
    "            img = cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)\n",
    "        elif len(image.shape) == 3 and image.shape[2] == 3:\n",
    "            img = image\n",
    "        elif len(image.shape) == 3 and image.shape[2] == 4:  # RGBAscale\n",
    "            img = image[:, :, :3]\n",
    "\n",
    "    return img\n",
    "\n",
    "def rectify_poly(img, poly):\n",
    "    # Use Affine transform\n",
    "    n = int(len(poly) / 2) - 1\n",
    "    width = 0\n",
    "    height = 0\n",
    "    for k in range(n):\n",
    "        box = np.float32([poly[k], poly[k + 1], poly[-k - 2], poly[-k - 1]])\n",
    "        width += int(\n",
    "            (np.linalg.norm(box[0] - box[1]) + np.linalg.norm(box[2] - box[3])) / 2\n",
    "        )\n",
    "        height += np.linalg.norm(box[1] - box[2])\n",
    "    width = int(width)\n",
    "    height = int(height / n)\n",
    "\n",
    "    output_img = np.zeros((height, width, 3), dtype=np.uint8)\n",
    "    width_step = 0\n",
    "    for k in range(n):\n",
    "        box = np.float32([poly[k], poly[k + 1], poly[-k - 2], poly[-k - 1]])\n",
    "        w = int((np.linalg.norm(box[0] - box[1]) + np.linalg.norm(box[2] - box[3])) / 2)\n",
    "\n",
    "        # Top triangle\n",
    "        pts1 = box[:3]\n",
    "        pts2 = np.float32(\n",
    "            [[width_step, 0], [width_step + w - 1, 0], [width_step + w - 1, height - 1]]\n",
    "        )\n",
    "        M = cv2.getAffineTransform(pts1, pts2)\n",
    "        warped_img = cv2.warpAffine(\n",
    "            img, M, (width, height), borderMode=cv2.BORDER_REPLICATE\n",
    "        )\n",
    "        warped_mask = np.zeros((height, width, 3), dtype=np.uint8)\n",
    "        warped_mask = cv2.fillConvexPoly(warped_mask, np.int32(pts2), (1, 1, 1))\n",
    "        output_img[warped_mask == 1] = warped_img[warped_mask == 1]\n",
    "\n",
    "        # Bottom triangle\n",
    "        pts1 = np.vstack((box[0], box[2:]))\n",
    "        pts2 = np.float32(\n",
    "            [\n",
    "                [width_step, 0],\n",
    "                [width_step + w - 1, height - 1],\n",
    "                [width_step, height - 1],\n",
    "            ]\n",
    "        )\n",
    "        M = cv2.getAffineTransform(pts1, pts2)\n",
    "        warped_img = cv2.warpAffine(\n",
    "            img, M, (width, height), borderMode=cv2.BORDER_REPLICATE\n",
    "        )\n",
    "        warped_mask = np.zeros((height, width, 3), dtype=np.uint8)\n",
    "        warped_mask = cv2.fillConvexPoly(warped_mask, np.int32(pts2), (1, 1, 1))\n",
    "        cv2.line(\n",
    "            warped_mask, (width_step, 0), (width_step + w - 1, height - 1), (0, 0, 0), 1)\n",
    "        output_img[warped_mask == 1] = warped_img[warped_mask == 1]\n",
    "\n",
    "        width_step += w\n",
    "    return output_img\n",
    "\n",
    "def crop_poly(image, poly):\n",
    "    # points should have 1*x*2  shape\n",
    "    if len(poly.shape) == 2:\n",
    "        poly = np.array([np.array(poly).astype(np.int32)])\n",
    "\n",
    "    # create mask with shape of image\n",
    "    mask = np.zeros(image.shape[0:2], dtype=np.uint8)\n",
    "\n",
    "    # method 1 smooth region\n",
    "    cv2.drawContours(mask, [poly], -1, (255, 255, 255), -1, cv2.LINE_AA)\n",
    "    # method 2 not so smooth region\n",
    "    # cv2.fillPoly(mask, points, (255))\n",
    "\n",
    "    # crop around poly\n",
    "    res = cv2.bitwise_and(image, image, mask=mask)\n",
    "    rect = cv2.boundingRect(poly)  # returns (x,y,w,h) of the rect\n",
    "    cropped = res[rect[1] : rect[1] + rect[3], rect[0] : rect[0] + rect[2]]\n",
    "    return cropped\n",
    "\n",
    "def export_detected_region(image, poly, rectify=True):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        image: full image\n",
    "        points: bbox or poly points\n",
    "        rectify: rectify detected polygon by affine transform\n",
    "    \"\"\"\n",
    "    if rectify:\n",
    "        # rectify poly region\n",
    "        result_rgb = rectify_poly(image, poly)\n",
    "    else:\n",
    "        result_rgb = crop_poly(image, poly)\n",
    "\n",
    "    # export corpped region\n",
    "    result_bgr = cv2.cvtColor(result_rgb, cv2.COLOR_RGB2BGR)\n",
    "    return result_bgr\n",
    "\n",
    "import copy\n",
    "\n",
    "def export_detected_regions(\n",
    "    image,\n",
    "    regions,\n",
    "    rectify: bool = False,\n",
    "):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        image: path to the image to be processed or numpy array or PIL image\n",
    "        regions: list of bboxes or polys\n",
    "        rectify: rectify detected polygon by affine transform\n",
    "    \"\"\"\n",
    "\n",
    "    # read/convert image\n",
    "    image = read_image(image)\n",
    "\n",
    "    # deepcopy image so that original is not altered\n",
    "    image = copy.deepcopy(image)\n",
    "\n",
    "    # init exported file paths\n",
    "    exported_images = []\n",
    "\n",
    "    # export regions\n",
    "    for ind, region in enumerate(regions):\n",
    "        # get export path\n",
    "        #file_path = os.path.join(crops_dir, \"crop_\" + str(ind) + \".png\")\n",
    "        # export region\n",
    "        crop = export_detected_region(image, poly=region, rectify=rectify)\n",
    "        # note exported file path\n",
    "        exported_images.append(crop)\n",
    "\n",
    "    return exported_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def recognizer_predict(converter, test_loader, batch_max_length,\\\n",
    "                       ignore_idx, char_group_idx, decoder='greedy', beamWidth=5, device='cpu'):\n",
    "    result = []\n",
    "    with torch.no_grad():\n",
    "        for image_tensors in test_loader:\n",
    "            batch_size = image_tensors.size(0)\n",
    "            image = image_tensors.to(device)\n",
    "            # print(image.shape)\n",
    "            # For max length prediction\n",
    "            length_for_pred = torch.IntTensor([batch_max_length] * batch_size).to(device)\n",
    "            text_for_pred = torch.LongTensor(batch_size, batch_max_length + 1).fill_(0).to(device)\n",
    "\n",
    "            #preds = model(image, text_for_pred)\n",
    "\n",
    "            providers = ['CPUExecutionProvider']\n",
    "            session = rt.InferenceSession(\"onnx_files/recog_en.onnx\", providers=providers)\n",
    "            inputs = session.get_inputs()\n",
    "\n",
    "            # print(len(inputs))\n",
    "\n",
    "            inp = {inputs[0].name: image.numpy()}\n",
    "            preds = session.run(None, inp)\n",
    "\n",
    "            preds = torch.from_numpy(preds[0])\n",
    "\n",
    "            # Select max probabilty (greedy decoding) then decode index to character\n",
    "            preds_size = torch.IntTensor([preds.size(1)] * batch_size)\n",
    "\n",
    "            ######## filter ignore_char, rebalance\n",
    "            preds_prob = F.softmax(preds, dim=2)\n",
    "            preds_prob = preds_prob.cpu().detach().numpy()\n",
    "            preds_prob[:,:,ignore_idx] = 0.\n",
    "            pred_norm = preds_prob.sum(axis=2)\n",
    "            preds_prob = preds_prob/np.expand_dims(pred_norm, axis=-1)\n",
    "            preds_prob = torch.from_numpy(preds_prob).float().to(device)\n",
    "\n",
    "            if decoder == 'greedy':\n",
    "                # Select max probabilty (greedy decoding) then decode index to character\n",
    "                _, preds_index = preds_prob.max(2)\n",
    "                preds_index = preds_index.view(-1)\n",
    "                preds_str = converter.decode_greedy(preds_index.data.cpu().detach().numpy(), preds_size.data)\n",
    "            elif decoder == 'beamsearch':\n",
    "                k = preds_prob.cpu().detach().numpy()\n",
    "                preds_str = converter.decode_beamsearch(k, beamWidth=beamWidth)\n",
    "            elif decoder == 'wordbeamsearch':\n",
    "                k = preds_prob.cpu().detach().numpy()\n",
    "                preds_str = converter.decode_wordbeamsearch(k, beamWidth=beamWidth)\n",
    "\n",
    "            preds_prob = preds_prob.cpu().detach().numpy()\n",
    "            values = preds_prob.max(axis=2)\n",
    "            indices = preds_prob.argmax(axis=2)\n",
    "            preds_max_prob = []\n",
    "            for v,i in zip(values, indices):\n",
    "                max_probs = v[i!=0]\n",
    "                if len(max_probs)>0:\n",
    "                    preds_max_prob.append(max_probs)\n",
    "                else:\n",
    "                    preds_max_prob.append(np.array([0]))\n",
    "\n",
    "            for pred, pred_max_prob in zip(preds_str, preds_max_prob):\n",
    "                confidence_score = recognition.custom_mean(pred_max_prob)\n",
    "                result.append([pred, confidence_score])\n",
    "\n",
    "    return result\n",
    "\n",
    "def get_text(character, imgH, imgW, converter, image_list,\\\n",
    "             ignore_char='', decoder='greedy', beamWidth=5, batch_size=1, contrast_ths=0.1,\\\n",
    "             adjust_contrast=0.5, filter_ths = 0.003, workers = 1, device = 'cpu'):\n",
    "    batch_max_length = int(imgW/10)\n",
    "\n",
    "    char_group_idx = {}\n",
    "    ignore_idx = []\n",
    "    for char in ignore_char:\n",
    "        try: ignore_idx.append(character.index(char)+1)\n",
    "        except: pass\n",
    "\n",
    "    coord = [item[0] for item in image_list]\n",
    "    img_list = [item[1] for item in image_list]\n",
    "    AlignCollate_normal = recognition.AlignCollate(imgH=imgH, imgW=imgW, keep_ratio_with_pad=True)\n",
    "    test_data = recognition.ListDataset(img_list)\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        test_data, batch_size=batch_size, shuffle=False,\n",
    "        num_workers=int(workers), collate_fn=AlignCollate_normal, pin_memory=True)\n",
    "\n",
    "    # predict first round\n",
    "    result1 = recognizer_predict(converter, test_loader,batch_max_length,\\\n",
    "                                 ignore_idx, char_group_idx, decoder, beamWidth, device = device)\n",
    "\n",
    "    # predict second round\n",
    "    low_confident_idx = [i for i,item in enumerate(result1) if (item[1] < contrast_ths)]\n",
    "    if len(low_confident_idx) > 0:\n",
    "        img_list2 = [img_list[i] for i in low_confident_idx]\n",
    "        AlignCollate_contrast = recognition.AlignCollate(imgH=imgH, imgW=imgW, keep_ratio_with_pad=True, adjust_contrast=adjust_contrast)\n",
    "        test_data = recognition.ListDataset(img_list2)\n",
    "        test_loader = torch.utils.data.DataLoader(\n",
    "                        test_data, batch_size=batch_size, shuffle=False,\n",
    "                        num_workers=int(workers), collate_fn=AlignCollate_contrast, pin_memory=True)\n",
    "        result2 = recognizer_predict(converter, test_loader, batch_max_length,\\\n",
    "                                     ignore_idx, char_group_idx, decoder, beamWidth, device = device)\n",
    "    result = []\n",
    "    for i, zipped in enumerate(zip(coord, result1)):\n",
    "        box, pred1 = zipped\n",
    "        if i in low_confident_idx:\n",
    "            pred2 = result2[low_confident_idx.index(i)]\n",
    "            if pred1[1]>pred2[1]:\n",
    "                result.append( (box, pred1[0], pred1[1]) )\n",
    "            else:\n",
    "                result.append( (box, pred2[0], pred2[1]) )\n",
    "        else:\n",
    "            result.append( (box, pred1[0], pred1[1]) )\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "recog_network = 'generation2'\n",
    "network_params = {\n",
    "    'input_channel': 1,\n",
    "    'output_channel': 256,\n",
    "    'hidden_size': 256}\n",
    "character = \"0123456789!\\\"#$%&'()*+,-./:;<=>?@[\\\\]^_`{|}~ €ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\"\n",
    "symbol = \"0123456789!\\\"#$%&'()*+,-./:;<=>?@[\\\\]^_`{|}~ €\"\n",
    "model_path = \"english_g2.pth\"\n",
    "separator_list = {}\n",
    "cyrillic_lang_list = ['en']\n",
    "package_dir = os.path.dirname(recognition.__file__)\n",
    "\n",
    "dict_list = {}\n",
    "for lang in cyrillic_lang_list:\n",
    "    dict_list[lang] = os.path.join(package_dir, 'dict', lang + \".txt\")\n",
    "\n",
    "batch_size = 1\n",
    "num_channels = 1\n",
    "image_height = imgH = 64\n",
    "image_width = 128\n",
    "lang_char = []\n",
    "for lang in cyrillic_lang_list:\n",
    "    char_file = os.path.join(package_dir, 'character', lang + \"_char.txt\")\n",
    "    with open(char_file, \"r\", encoding = \"utf-8-sig\") as input_file:\n",
    "        char_list =  input_file.read().splitlines()\n",
    "    lang_char += char_list\n",
    "\n",
    "lang_char = set(lang_char).union(set(symbol))\n",
    "ignore_char = ''.join(set(character)-set(lang_char))\n",
    "\n",
    "# read image\n",
    "converter = recognition.CTCLabelConverter(character, separator_list, dict_list)\n",
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'en': 'C:\\\\Users\\\\dimaz\\\\AppData\\\\Local\\\\Packages\\\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\\\LocalCache\\\\local-packages\\\\Python310\\\\site-packages\\\\easyocr_onnx\\\\dict\\\\en.txt'}"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\dimaz\\\\AppData\\\\Local\\\\Packages\\\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\\\LocalCache\\\\local-packages\\\\Python310\\\\site-packages\\\\easyocr_onnx\\\\__init__.py'"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "easyocr_onnx.__file__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'images/6.jpg'\n",
    "def easyocr_onnx_inference(filename):\n",
    "    image = Image.open(filename)\n",
    "    if image.size[0]>image.size[1]: image = image.rotate(270)\n",
    "    box_terminal = model_terminal(image, verbose=False)\n",
    "    x1, y1, x2, y2 = box_terminal[0].boxes.xyxy[0].numpy()\n",
    "    image = image.crop((x1, y1, x2, y2))\n",
    "    img, _ = reformat_input(np.array(image))\n",
    "    # Resize and normalize input image\n",
    "    img_resized, target_ratio, size_heatmap = resize_aspect_ratio(img, 512, interpolation=cv2.INTER_LINEAR, mag_ratio=1.)\n",
    "    ratio_h = ratio_w = 1 / target_ratio\n",
    "    x = normalizeMeanVariance(img_resized)\n",
    "    x = torch.from_numpy(x).permute(2, 0, 1).unsqueeze(0)\n",
    "    # Create ONNX Runtime session and load model\n",
    "    providers = ['CPUExecutionProvider']\n",
    "    session = rt.InferenceSession(\"onnx_files/craft_en.onnx\", providers=providers)\n",
    "    input_name = session.get_inputs()[0].name\n",
    "    # Prepare input tensor for inference\n",
    "    inp = {input_name: x.numpy()}\n",
    "    # Run inference and get output\n",
    "    y, _ = session.run(None, inp)\n",
    "    # Extract score and link maps\n",
    "    score_text = y[0, :, :, 0]\n",
    "    score_link = y[0, :, :, 1]\n",
    "    # Post-processing to obtain bounding boxes and polygons\n",
    "    boxes, polys, mapper = getDetBoxes(score_text, score_link, 0.5, 0.4, 0.4)\n",
    "    boxes = adjustResultCoordinates(boxes, ratio_w, ratio_h)\n",
    "    crops = export_detected_regions(\n",
    "        image=img,\n",
    "        regions=boxes,\n",
    "        rectify=True)\n",
    "    result = []\n",
    "    for crop in crops:\n",
    "        img, img_cv_grey = reformat_input(crop)\n",
    "        y_max, x_max = img_cv_grey.shape\n",
    "        horizontal_list = [[0, x_max, 0, y_max]]\n",
    "        for bbox in horizontal_list:\n",
    "            h_list = [bbox]\n",
    "            f_list = []\n",
    "            image_list, max_width = get_image_list(h_list, f_list, img_cv_grey, model_height=64) # 64 is default value\n",
    "            result0 = get_text(character, imgH, int(max_width), converter, image_list,\\\n",
    "                                    ignore_char, 'greedy', beamWidth=5, batch_size=batch_size, \n",
    "                                    contrast_ths=0.1, adjust_contrast=0.5, filter_ths=0.003,\\\n",
    "                                    workers=0, device=device)\n",
    "            result += result0\n",
    "    char = []\n",
    "    for i in range(len(result)):\n",
    "        char.append(result[i][1])\n",
    "    return char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:19<00:00,  1.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 54.4 s\n",
      "Wall time: 19.2 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dict_results_onnx_en = {} \n",
    "for filename in tqdm(images):\n",
    "    dict_results_onnx_en[filename] = easyocr_onnx_inference(filename)\n",
    "    # chars.append(easyocr_onnx_inference(filename))\n",
    "# chars = [easyocr_onnx_inference(filename) for filename in tqdm(images)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "lists_onnx_en = []\n",
    "for row in list(dict_results_onnx_en.values()):\n",
    "    lists_onnx_en.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_onnx_en = pd.DataFrame({\n",
    "    'filename': list(dict_results_onnx_en.keys()),\n",
    "    'ocr_rows_onnx_en': lists_onnx_en})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result = df_result.merge(df_onnx_en, left_on='filename', right_on='filename')\n",
    "onnx_flags_en = []\n",
    "for i, tid in enumerate(correct_tids):\n",
    "    onnx_flags_en.append(tid in df_result['ocr_rows_onnx_en'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(onnx_flags_en)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Terminal OD + models quant ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from onnxruntime.quantization import quantize_dynamic, QuantType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Please consider to run pre-processing before quantization. Refer to example: https://github.com/microsoft/onnxruntime-inference-examples/blob/main/quantization/image_classification/cpu/ReadMe.md \n"
     ]
    }
   ],
   "source": [
    "# craft_fp32 = 'onnx_files/craft_en.onnx'\n",
    "# craft_quant = 'onnx_files/craft_en_quant.onnx' \n",
    "# model_craft_quant = quantize_dynamic(craft_fp32, craft_quant,\n",
    "#                                      nodes_to_exclude=['Conv_quant'])\n",
    "# # weight_type=QuantType.QInt8, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "from onnxsim import simplify\n",
    "\n",
    "# load your predefined ONNX model\n",
    "model = onnx.load('onnx_files/craft_en.onnx')\n",
    "\n",
    "# convert model\n",
    "model_simp, check = simplify(model)\n",
    "\n",
    "assert check, \"Simplified ONNX model could not be validated\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx.save_model(model_simp, 'onnx_files/craft_en_onnxsim.onnx' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Please consider to run pre-processing before quantization. Refer to example: https://github.com/microsoft/onnxruntime-inference-examples/blob/main/quantization/image_classification/cpu/ReadMe.md \n"
     ]
    }
   ],
   "source": [
    "recog_fp32 = 'onnx_files/recog_en.onnx'\n",
    "recog_quant = 'onnx_files/recog_en_quant.onnx' \n",
    "# model_recog_quant = quantize_dynamic(recog_fp32, recog_quant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = onnx.load('onnx_files/recog_en.onnx')\n",
    "# convert model\n",
    "model_simp, check = simplify(model)\n",
    "\n",
    "assert check, \"Simplified ONNX model could not be validated\"\n",
    "onnx.save_model(model_simp, 'onnx_files/recog_en_onnxsim.onnx' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import onnxruntime as rt\n",
    "from easyocr_onnx.craft_utils import getDetBoxes, adjustResultCoordinates\n",
    "from easyocr_onnx.imgproc import resize_aspect_ratio, normalizeMeanVariance\n",
    "from easyocr_onnx.utils import reformat_input, get_image_list\n",
    "from easyocr_onnx import recognition\n",
    "\n",
    "def read_image(image):\n",
    "    if type(image) == str:\n",
    "        img = cv2.imread(image)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    elif type(image) == bytes:\n",
    "        nparr = np.frombuffer(image, np.uint8)\n",
    "        img = cv2.imdecode(nparr, cv2.IMREAD_COLOR)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    elif type(image) == np.ndarray:\n",
    "        if len(image.shape) == 2:  # grayscale\n",
    "            img = cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)\n",
    "        elif len(image.shape) == 3 and image.shape[2] == 3:\n",
    "            img = image\n",
    "        elif len(image.shape) == 3 and image.shape[2] == 4:  # RGBAscale\n",
    "            img = image[:, :, :3]\n",
    "\n",
    "    return img\n",
    "\n",
    "def rectify_poly(img, poly):\n",
    "    # Use Affine transform\n",
    "    n = int(len(poly) / 2) - 1\n",
    "    width = 0\n",
    "    height = 0\n",
    "    for k in range(n):\n",
    "        box = np.float32([poly[k], poly[k + 1], poly[-k - 2], poly[-k - 1]])\n",
    "        width += int(\n",
    "            (np.linalg.norm(box[0] - box[1]) + np.linalg.norm(box[2] - box[3])) / 2\n",
    "        )\n",
    "        height += np.linalg.norm(box[1] - box[2])\n",
    "    width = int(width)\n",
    "    height = int(height / n)\n",
    "\n",
    "    output_img = np.zeros((height, width, 3), dtype=np.uint8)\n",
    "    width_step = 0\n",
    "    for k in range(n):\n",
    "        box = np.float32([poly[k], poly[k + 1], poly[-k - 2], poly[-k - 1]])\n",
    "        w = int((np.linalg.norm(box[0] - box[1]) + np.linalg.norm(box[2] - box[3])) / 2)\n",
    "\n",
    "        # Top triangle\n",
    "        pts1 = box[:3]\n",
    "        pts2 = np.float32(\n",
    "            [[width_step, 0], [width_step + w - 1, 0], [width_step + w - 1, height - 1]]\n",
    "        )\n",
    "        M = cv2.getAffineTransform(pts1, pts2)\n",
    "        warped_img = cv2.warpAffine(\n",
    "            img, M, (width, height), borderMode=cv2.BORDER_REPLICATE\n",
    "        )\n",
    "        warped_mask = np.zeros((height, width, 3), dtype=np.uint8)\n",
    "        warped_mask = cv2.fillConvexPoly(warped_mask, np.int32(pts2), (1, 1, 1))\n",
    "        output_img[warped_mask == 1] = warped_img[warped_mask == 1]\n",
    "\n",
    "        # Bottom triangle\n",
    "        pts1 = np.vstack((box[0], box[2:]))\n",
    "        pts2 = np.float32(\n",
    "            [\n",
    "                [width_step, 0],\n",
    "                [width_step + w - 1, height - 1],\n",
    "                [width_step, height - 1],\n",
    "            ]\n",
    "        )\n",
    "        M = cv2.getAffineTransform(pts1, pts2)\n",
    "        warped_img = cv2.warpAffine(\n",
    "            img, M, (width, height), borderMode=cv2.BORDER_REPLICATE\n",
    "        )\n",
    "        warped_mask = np.zeros((height, width, 3), dtype=np.uint8)\n",
    "        warped_mask = cv2.fillConvexPoly(warped_mask, np.int32(pts2), (1, 1, 1))\n",
    "        cv2.line(\n",
    "            warped_mask, (width_step, 0), (width_step + w - 1, height - 1), (0, 0, 0), 1)\n",
    "        output_img[warped_mask == 1] = warped_img[warped_mask == 1]\n",
    "\n",
    "        width_step += w\n",
    "    return output_img\n",
    "\n",
    "def crop_poly(image, poly):\n",
    "    # points should have 1*x*2  shape\n",
    "    if len(poly.shape) == 2:\n",
    "        poly = np.array([np.array(poly).astype(np.int32)])\n",
    "\n",
    "    # create mask with shape of image\n",
    "    mask = np.zeros(image.shape[0:2], dtype=np.uint8)\n",
    "\n",
    "    # method 1 smooth region\n",
    "    cv2.drawContours(mask, [poly], -1, (255, 255, 255), -1, cv2.LINE_AA)\n",
    "    # method 2 not so smooth region\n",
    "    # cv2.fillPoly(mask, points, (255))\n",
    "\n",
    "    # crop around poly\n",
    "    res = cv2.bitwise_and(image, image, mask=mask)\n",
    "    rect = cv2.boundingRect(poly)  # returns (x,y,w,h) of the rect\n",
    "    cropped = res[rect[1] : rect[1] + rect[3], rect[0] : rect[0] + rect[2]]\n",
    "    return cropped\n",
    "\n",
    "def export_detected_region(image, poly, rectify=True):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        image: full image\n",
    "        points: bbox or poly points\n",
    "        rectify: rectify detected polygon by affine transform\n",
    "    \"\"\"\n",
    "    if rectify:\n",
    "        # rectify poly region\n",
    "        result_rgb = rectify_poly(image, poly)\n",
    "    else:\n",
    "        result_rgb = crop_poly(image, poly)\n",
    "\n",
    "    # export corpped region\n",
    "    result_bgr = cv2.cvtColor(result_rgb, cv2.COLOR_RGB2BGR)\n",
    "    return result_bgr\n",
    "\n",
    "import copy\n",
    "\n",
    "def export_detected_regions(\n",
    "    image,\n",
    "    regions,\n",
    "    rectify: bool = False,\n",
    "):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        image: path to the image to be processed or numpy array or PIL image\n",
    "        regions: list of bboxes or polys\n",
    "        rectify: rectify detected polygon by affine transform\n",
    "    \"\"\"\n",
    "\n",
    "    # read/convert image\n",
    "    image = read_image(image)\n",
    "\n",
    "    # deepcopy image so that original is not altered\n",
    "    image = copy.deepcopy(image)\n",
    "\n",
    "    # init exported file paths\n",
    "    exported_images = []\n",
    "\n",
    "    # export regions\n",
    "    for ind, region in enumerate(regions):\n",
    "        # get export path\n",
    "        #file_path = os.path.join(crops_dir, \"crop_\" + str(ind) + \".png\")\n",
    "        # export region\n",
    "        crop = export_detected_region(image, poly=region, rectify=rectify)\n",
    "        # note exported file path\n",
    "        exported_images.append(crop)\n",
    "\n",
    "    return exported_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def recognizer_predict(converter, test_loader, batch_max_length,\\\n",
    "                       ignore_idx, char_group_idx, decoder='greedy', beamWidth=5, device='cpu'):\n",
    "    result = []\n",
    "    with torch.no_grad():\n",
    "        for image_tensors in test_loader:\n",
    "            batch_size = image_tensors.size(0)\n",
    "            image = image_tensors.to(device)\n",
    "            # print(image.shape)\n",
    "            # For max length prediction\n",
    "            length_for_pred = torch.IntTensor([batch_max_length] * batch_size).to(device)\n",
    "            text_for_pred = torch.LongTensor(batch_size, batch_max_length + 1).fill_(0).to(device)\n",
    "\n",
    "            #preds = model(image, text_for_pred)\n",
    "\n",
    "            providers = ['CPUExecutionProvider']\n",
    "            session = rt.InferenceSession(\"onnx_files/recog_en_onnxsim.onnx\", providers=providers)\n",
    "            inputs = session.get_inputs()\n",
    "\n",
    "            # print(len(inputs))\n",
    "\n",
    "            inp = {inputs[0].name: image.numpy()}\n",
    "            preds = session.run(None, inp)\n",
    "\n",
    "            preds = torch.from_numpy(preds[0])\n",
    "\n",
    "            # Select max probabilty (greedy decoding) then decode index to character\n",
    "            preds_size = torch.IntTensor([preds.size(1)] * batch_size)\n",
    "\n",
    "            ######## filter ignore_char, rebalance\n",
    "            preds_prob = F.softmax(preds, dim=2)\n",
    "            preds_prob = preds_prob.cpu().detach().numpy()\n",
    "            preds_prob[:,:,ignore_idx] = 0.\n",
    "            pred_norm = preds_prob.sum(axis=2)\n",
    "            preds_prob = preds_prob/np.expand_dims(pred_norm, axis=-1)\n",
    "            preds_prob = torch.from_numpy(preds_prob).float().to(device)\n",
    "\n",
    "            if decoder == 'greedy':\n",
    "                # Select max probabilty (greedy decoding) then decode index to character\n",
    "                _, preds_index = preds_prob.max(2)\n",
    "                preds_index = preds_index.view(-1)\n",
    "                preds_str = converter.decode_greedy(preds_index.data.cpu().detach().numpy(), preds_size.data)\n",
    "            elif decoder == 'beamsearch':\n",
    "                k = preds_prob.cpu().detach().numpy()\n",
    "                preds_str = converter.decode_beamsearch(k, beamWidth=beamWidth)\n",
    "            elif decoder == 'wordbeamsearch':\n",
    "                k = preds_prob.cpu().detach().numpy()\n",
    "                preds_str = converter.decode_wordbeamsearch(k, beamWidth=beamWidth)\n",
    "\n",
    "            preds_prob = preds_prob.cpu().detach().numpy()\n",
    "            values = preds_prob.max(axis=2)\n",
    "            indices = preds_prob.argmax(axis=2)\n",
    "            preds_max_prob = []\n",
    "            for v,i in zip(values, indices):\n",
    "                max_probs = v[i!=0]\n",
    "                if len(max_probs)>0:\n",
    "                    preds_max_prob.append(max_probs)\n",
    "                else:\n",
    "                    preds_max_prob.append(np.array([0]))\n",
    "\n",
    "            for pred, pred_max_prob in zip(preds_str, preds_max_prob):\n",
    "                confidence_score = recognition.custom_mean(pred_max_prob)\n",
    "                result.append([pred, confidence_score])\n",
    "\n",
    "    return result\n",
    "\n",
    "def get_text(character, imgH, imgW, converter, image_list,\\\n",
    "             ignore_char='', decoder='greedy', beamWidth=5, batch_size=1, contrast_ths=0.1,\\\n",
    "             adjust_contrast=0.5, filter_ths = 0.003, workers = 1, device = 'cpu'):\n",
    "    batch_max_length = int(imgW/10)\n",
    "\n",
    "    char_group_idx = {}\n",
    "    ignore_idx = []\n",
    "    for char in ignore_char:\n",
    "        try: ignore_idx.append(character.index(char)+1)\n",
    "        except: pass\n",
    "\n",
    "    coord = [item[0] for item in image_list]\n",
    "    img_list = [item[1] for item in image_list]\n",
    "    AlignCollate_normal = recognition.AlignCollate(imgH=imgH, imgW=imgW, keep_ratio_with_pad=True)\n",
    "    test_data = recognition.ListDataset(img_list)\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        test_data, batch_size=batch_size, shuffle=False,\n",
    "        num_workers=int(workers), collate_fn=AlignCollate_normal, pin_memory=True)\n",
    "\n",
    "    # predict first round\n",
    "    result1 = recognizer_predict(converter, test_loader,batch_max_length,\\\n",
    "                                 ignore_idx, char_group_idx, decoder, beamWidth, device = device)\n",
    "\n",
    "    # predict second round\n",
    "    low_confident_idx = [i for i,item in enumerate(result1) if (item[1] < contrast_ths)]\n",
    "    if len(low_confident_idx) > 0:\n",
    "        img_list2 = [img_list[i] for i in low_confident_idx]\n",
    "        AlignCollate_contrast = recognition.AlignCollate(imgH=imgH, imgW=imgW, keep_ratio_with_pad=True, adjust_contrast=adjust_contrast)\n",
    "        test_data = recognition.ListDataset(img_list2)\n",
    "        test_loader = torch.utils.data.DataLoader(\n",
    "                        test_data, batch_size=batch_size, shuffle=False,\n",
    "                        num_workers=int(workers), collate_fn=AlignCollate_contrast, pin_memory=True)\n",
    "        result2 = recognizer_predict(converter, test_loader, batch_max_length,\\\n",
    "                                     ignore_idx, char_group_idx, decoder, beamWidth, device = device)\n",
    "    result = []\n",
    "    for i, zipped in enumerate(zip(coord, result1)):\n",
    "        box, pred1 = zipped\n",
    "        if i in low_confident_idx:\n",
    "            pred2 = result2[low_confident_idx.index(i)]\n",
    "            if pred1[1]>pred2[1]:\n",
    "                result.append( (box, pred1[0], pred1[1]) )\n",
    "            else:\n",
    "                result.append( (box, pred2[0], pred2[1]) )\n",
    "        else:\n",
    "            result.append( (box, pred1[0], pred1[1]) )\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "recog_network = 'generation2'\n",
    "network_params = {\n",
    "    'input_channel': 1,\n",
    "    'output_channel': 256,\n",
    "    'hidden_size': 256}\n",
    "character = \"0123456789!\\\"#$%&'()*+,-./:;<=>?@[\\\\]^_`{|}~ €ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\"\n",
    "symbol = \"0123456789!\\\"#$%&'()*+,-./:;<=>?@[\\\\]^_`{|}~ €\"\n",
    "model_path = \"english_g2.pth\"\n",
    "separator_list = {}\n",
    "cyrillic_lang_list = ['en']\n",
    "package_dir = os.path.dirname(recognition.__file__)\n",
    "\n",
    "dict_list = {}\n",
    "for lang in cyrillic_lang_list:\n",
    "    dict_list[lang] = os.path.join(package_dir, 'dict', lang + \".txt\")\n",
    "\n",
    "batch_size = 1\n",
    "num_channels = 1\n",
    "image_height = imgH = 64\n",
    "image_width = 128\n",
    "lang_char = []\n",
    "for lang in cyrillic_lang_list:\n",
    "    char_file = os.path.join(package_dir, 'character', lang + \"_char.txt\")\n",
    "    with open(char_file, \"r\", encoding = \"utf-8-sig\") as input_file:\n",
    "        char_list =  input_file.read().splitlines()\n",
    "    lang_char += char_list\n",
    "\n",
    "lang_char = set(lang_char).union(set(symbol))\n",
    "ignore_char = ''.join(set(character)-set(lang_char))\n",
    "\n",
    "# read image\n",
    "converter = recognition.CTCLabelConverter(character, separator_list, dict_list)\n",
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'en': 'C:\\\\Users\\\\dimaz\\\\AppData\\\\Local\\\\Packages\\\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\\\LocalCache\\\\local-packages\\\\Python310\\\\site-packages\\\\easyocr_onnx\\\\dict\\\\en.txt'}"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\dimaz\\\\AppData\\\\Local\\\\Packages\\\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\\\LocalCache\\\\local-packages\\\\Python310\\\\site-packages\\\\easyocr_onnx\\\\__init__.py'"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "easyocr_onnx.__file__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'images/6.jpg'\n",
    "def easyocr_onnx_inference(filename):\n",
    "    image = Image.open(filename)\n",
    "    if image.size[0]>image.size[1]: image = image.rotate(270)\n",
    "    box_terminal = model_terminal(image, verbose=False)\n",
    "    x1, y1, x2, y2 = box_terminal[0].boxes.xyxy[0].numpy()\n",
    "    image = image.crop((x1, y1, x2, y2))\n",
    "    img, _ = reformat_input(np.array(image))\n",
    "    # Resize and normalize input image\n",
    "    img_resized, target_ratio, size_heatmap = resize_aspect_ratio(img, 512, interpolation=cv2.INTER_LINEAR, mag_ratio=1.)\n",
    "    ratio_h = ratio_w = 1 / target_ratio\n",
    "    x = normalizeMeanVariance(img_resized)\n",
    "    x = torch.from_numpy(x).permute(2, 0, 1).unsqueeze(0)\n",
    "    # Create ONNX Runtime session and load model\n",
    "    providers = ['CPUExecutionProvider']\n",
    "    session = rt.InferenceSession(\"onnx_files/craft_en_onnxsim.onnx\", providers=providers)\n",
    "    input_name = session.get_inputs()[0].name\n",
    "    # Prepare input tensor for inference\n",
    "    inp = {input_name: x.numpy()}\n",
    "    # Run inference and get output\n",
    "    y, _ = session.run(None, inp)\n",
    "    # Extract score and link maps\n",
    "    score_text = y[0, :, :, 0]\n",
    "    score_link = y[0, :, :, 1]\n",
    "    # Post-processing to obtain bounding boxes and polygons\n",
    "    boxes, polys, mapper = getDetBoxes(score_text, score_link, 0.5, 0.4, 0.4)\n",
    "    boxes = adjustResultCoordinates(boxes, ratio_w, ratio_h)\n",
    "    crops = export_detected_regions(\n",
    "        image=img,\n",
    "        regions=boxes,\n",
    "        rectify=True)\n",
    "    result = []\n",
    "    for crop in crops:\n",
    "        img, img_cv_grey = reformat_input(crop)\n",
    "        y_max, x_max = img_cv_grey.shape\n",
    "        horizontal_list = [[0, x_max, 0, y_max]]\n",
    "        for bbox in horizontal_list:\n",
    "            h_list = [bbox]\n",
    "            f_list = []\n",
    "            image_list, max_width = get_image_list(h_list, f_list, img_cv_grey, model_height=64) # 64 is default value\n",
    "            result0 = get_text(character, imgH, int(max_width), converter, image_list,\\\n",
    "                                    ignore_char, 'greedy', beamWidth=5, batch_size=batch_size, \n",
    "                                    contrast_ths=0.1, adjust_contrast=0.5, filter_ths=0.003,\\\n",
    "                                    workers=0, device=device)\n",
    "            result += result0\n",
    "    char = []\n",
    "    for i in range(len(result)):\n",
    "        char.append(result[i][1])\n",
    "    return char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:19<00:00,  1.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 1min 2s\n",
      "Wall time: 19.9 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dict_results_onnx_en_sim = {} \n",
    "for filename in tqdm(images):\n",
    "    dict_results_onnx_en_sim[filename] = easyocr_onnx_inference(filename)\n",
    "    # chars.append(easyocr_onnx_inference(filename))\n",
    "# chars = [easyocr_onnx_inference(filename) for filename in tqdm(images)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "lists_onnx_en_quant = []\n",
    "for row in list(dict_results_onnx_en_sim.values()):\n",
    "    lists_onnx_en_quant.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_onnx_en_sim = pd.DataFrame({\n",
    "    'filename': list(dict_results_onnx_en_sim.keys()),\n",
    "    'ocr_rows_onnx_en_sim': lists_onnx_en_quant})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result = df_result.merge(df_onnx_en_sim, left_on='filename', right_on='filename')\n",
    "onnx_flags_en_sim = []\n",
    "for i, tid in enumerate(correct_tids):\n",
    "    onnx_flags_en_sim.append(tid in df_result['ocr_rows_onnx_en_sim'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(onnx_flags_en_sim)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
